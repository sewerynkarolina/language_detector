{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import pdftotext\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAN DZIAŁANIA <br>\n",
    "1. Tworzę Counter ze wszystkimi n-gramami (o danej długości i o danych licznościach) <br>\n",
    "2. Tworzę listę ze wszystkimi wyrazami które są w Counterze <br>\n",
    "3. Dla każdego wyrazu wyliczam o ile za dużo jest go w Counterze i tą liczbę potem będę odejmować w macierzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text, n=2):\n",
    "    '''\n",
    "    wymagania: modul re\n",
    "    text - artykul, na podstawie ktorego chcemy uzyskac liste n gramow\n",
    "    zalozenie - 1.text zostal wczesniej poddany zabiegom lematyzacji i czyszczenia\n",
    "                2.type(text)==str\n",
    "    n>2, type(n)==int\n",
    "    '''\n",
    "    if type(text) != str:\n",
    "        raise NameError(\"Zly typ argumentu text, text musi byc typu str\")\n",
    "    if type(n) != int or n<2:\n",
    "        raise NameError(\"n musi byc liczba calkowita, n>2\")\n",
    "    \n",
    "    \n",
    "    text = text.lower()\n",
    "    tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text).split() #slowa z text zapisane do listy\n",
    "    \n",
    "    n_grams=list() #miejsce na n gramy\n",
    "    \n",
    "    for i in range(len(tokens)-n+1):\n",
    "        s = tokens[i] \n",
    "        for j in range(1,n):\n",
    "            s = s + \"_\" + tokens[i+j]\n",
    "        n_grams.append(s)\n",
    "    \n",
    "    return n_grams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/home/kasia/Pulpit/LD_data/HoangLongThuy.pdf'] #Tu będę miała pobraną "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Element listy zawiera pełny tekst jednego z artykułów\n",
    "text_list = []\n",
    "for text in file_paths:\n",
    "    with open(text, \"rb\") as f:\n",
    "        pdf = pdftotext.PDF(f)\n",
    "        el_of_list = ''\n",
    "        #Ponieważ page in pdf  - to jest strona z artykułu to łącze stringi, pewnie to można lepiej\n",
    "        for page in pdf:\n",
    "            el_of_list = el_of_list+page\n",
    "        text_list.append(el_of_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zakładam długość ngramu=7 \n",
    "n_gram = 7\n",
    "#Zakładam, że liczba powtórzeń tych n-gramów to co najmniej 4\n",
    "count_n_gram=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'eai_endorsed_transactions_on_scalable_information_systems': 6,\n",
       "         'research_on_innovating_applying_multiple_paths_routing': 4,\n",
       "         'on_innovating_applying_multiple_paths_routing_technique': 4,\n",
       "         'innovating_applying_multiple_paths_routing_technique_based': 4,\n",
       "         'applying_multiple_paths_routing_technique_based_on': 4,\n",
       "         'multiple_paths_routing_technique_based_on_fuzzy': 4,\n",
       "         'paths_routing_technique_based_on_fuzzy_logic': 4,\n",
       "         'routing_technique_based_on_fuzzy_logic_and': 4,\n",
       "         'technique_based_on_fuzzy_logic_and_genetic': 4,\n",
       "         'based_on_fuzzy_logic_and_genetic_algorithm': 4,\n",
       "         'on_fuzzy_logic_and_genetic_algorithm_for': 4,\n",
       "         'fuzzy_logic_and_genetic_algorithm_for_routing': 4,\n",
       "         'logic_and_genetic_algorithm_for_routing_messages': 4,\n",
       "         'and_genetic_algorithm_for_routing_messages_in': 4,\n",
       "         'genetic_algorithm_for_routing_messages_in_service': 4,\n",
       "         'algorithm_for_routing_messages_in_service_oriented': 4,\n",
       "         'for_routing_messages_in_service_oriented_routing': 4,\n",
       "         '01_02_2015_volume_2_issue_4': 8,\n",
       "         '02_2015_volume_2_issue_4_e2': 8,\n",
       "         '2015_volume_2_issue_4_e2_n': 4,\n",
       "         'volume_2_issue_4_e2_n_t': 4,\n",
       "         '2_issue_4_e2_n_t_long': 4,\n",
       "         'issue_4_e2_n_t_long_et': 4,\n",
       "         '4_e2_n_t_long_et_al': 4,\n",
       "         'scalable_information_systems_01_02_2015_volume': 6,\n",
       "         'information_systems_01_02_2015_volume_2': 6,\n",
       "         'systems_01_02_2015_volume_2_issue': 6})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Główny słownik\n",
    "main_dict = Counter(dict(filter(lambda x: x[1] >=count_n_gram, Counter(n_grams(text_list[0],n_gram)).items())))\n",
    "main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbieram teraz wszystkie wyrazy z countera\n",
    "stri =\"\"\n",
    "words_list =[]\n",
    "for i in np.unique(list(main_dict.elements())):\n",
    "    #Ta podłoga bo tak są połączone wyrazy\n",
    "    stri = stri + i + \"_\"\n",
    "words_list = np.unique(stri.split(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '01', '02', '2', '2015', '4', 'al', 'algorithm', 'and',\n",
       "       'applying', 'based', 'e2', 'eai', 'endorsed', 'et', 'for', 'fuzzy',\n",
       "       'genetic', 'in', 'information', 'innovating', 'issue', 'logic',\n",
       "       'long', 'messages', 'multiple', 'n', 'on', 'oriented', 'paths',\n",
       "       'research', 'routing', 'scalable', 'service', 'systems', 't',\n",
       "       'technique', 'transactions', 'volume'], dtype='<U12')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Teraz dla każdego wyrazu z words_list policzę o ile za dużo występuje w Counterze\n",
    "words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_sub = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stworzę nowy counter z elementami, które zawierają i:\n",
    "for i in words_list[1:]:\n",
    "\n",
    "    count_i = Counter()\n",
    "    for j in main_dict:\n",
    "            #Żeby nie policzyło np. dla in: routing\n",
    "        if (\"_\"+i+\"_\" in j or (i[-len(i):-1]==j[-len(i):-1] and i[-1]==j[-1])or i[0:len(i)]+\"_\"==j[0:len(i)+1] ):\n",
    "            count_i[j] = main_dict[j]\n",
    "        #count_i - counter na którym pracuję\n",
    "    counter_values= list(range(count_n_gram,max(count_i.items(), key=lambda x: x[1])[1]+1))\n",
    "\n",
    "    lista_counterow = []\n",
    "        #Długości powyższych counterów\n",
    "    len_counter = []\n",
    "    count_val = []\n",
    "\n",
    "    iterator = 0\n",
    "    for k in counter_values:\n",
    "        if  Counter(dict(filter(lambda x: x[1] ==k, count_i.items())))!=Counter():\n",
    "            lista_counterow.append(Counter(dict(filter(lambda x: x[1] == k, count_i.items()))))\n",
    "\n",
    "            len_counter.append(len(lista_counterow[iterator]))\n",
    "            count_val.append(k)\n",
    "            iterator += 1\n",
    "\n",
    "\n",
    "    list_to_mult = [x - 1 for x in count_val]\n",
    "    new = np.array(list_to_mult)*np.array(len_counter)\n",
    "    N = sum(np.array(count_val)*np.array(len_counter))\n",
    "    \n",
    "    num_to_sub[i]=N\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'01': 26,\n",
       " '02': 34,\n",
       " '2': 40,\n",
       " '2015': 38,\n",
       " '4': 36,\n",
       " 'al': 4,\n",
       " 'algorithm': 28,\n",
       " 'and': 28,\n",
       " 'applying': 16,\n",
       " 'based': 28,\n",
       " 'e2': 28,\n",
       " 'eai': 6,\n",
       " 'endorsed': 6,\n",
       " 'et': 8,\n",
       " 'for': 28,\n",
       " 'fuzzy': 28,\n",
       " 'genetic': 28,\n",
       " 'in': 16,\n",
       " 'information': 18,\n",
       " 'innovating': 12,\n",
       " 'issue': 38,\n",
       " 'logic': 28,\n",
       " 'long': 12,\n",
       " 'messages': 20,\n",
       " 'multiple': 20,\n",
       " 'n': 28,\n",
       " 'on': 42,\n",
       " 'oriented': 8,\n",
       " 'paths': 24,\n",
       " 'research': 4,\n",
       " 'routing': 52,\n",
       " 'scalable': 12,\n",
       " 'service': 12,\n",
       " 'systems': 24,\n",
       " 't': 16,\n",
       " 'technique': 28,\n",
       " 'transactions': 6,\n",
       " 'volume': 42}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
